# Plan for Qortal MCP Server Public Node Fallback

## 1. Audit of Current Qortal API Client Implementation

Base URL Configuration: The MCP server currently uses a single Qortal API endpoint defined in the configuration. By default, DEFAULT_BASE_URL is http://localhost:12391 (the local node’s API port), unless overridden by the environment variable QORTAL_BASE_URL[1]. This base URL is stored in a global QortalConfig (default_config) and includes other settings like a default HTTP timeout (10 seconds unless QORTAL_HTTP_TIMEOUT env var is set)[2]. The Qortal API key (if required for admin endpoints) is also loaded once from env or apikey.txt at startup[3][4].

HTTP Client Logic: The QortalApiClient is a thin async HTTP client wrapper (using httpx). On first use, it creates an AsyncClient with the configured base URL and timeout[5]. All subsequent requests use this single client instance, meaning all HTTP calls target the one configured base URL. The client provides coroutine methods like fetch_name_info, fetch_block_height, etc., which internally call a unified _request helper. This _request appends the path to the base URL and performs an HTTP GET, mapping any HTTP errors to internal exceptions[6][7]. Notably, any network failure (connection error, timeout, DNS failure, etc.) raises a NodeUnreachableError[6]. There is no retry or alternate host logic – a single failure results in an immediate error.

Global Usage Across Tools: A singleton default_client is instantiated at import time[8] using the default config (and thus the single base URL). All tool functions (in qortal_mcp.tools) accept a client parameter defaulting to this default_client and use it for Qortal API calls[9][10]. For example, get_name_info calls client.fetch_name_info(name) and catches exceptions. Each tool catches specific exceptions (InvalidAddressError, NameNotFoundError, etc.) and ultimately catches NodeUnreachableError to return {"error": "Node unreachable"} to the caller[11]. This pattern is consistent: if the sole configured node is down or unreachable, every tool call will yield a “Node unreachable” error with no further attempts.

Assumptions & Potential Issues for Multi-Node Support: The current design assumes only one Qortal backend node. Introducing multiple nodes (local + public fallback) will break certain assumptions:

- Single Base URL: The AsyncClient is bound to one base URL for all requests. We cannot simply “change” this base on the fly. The code will need refactoring to handle multiple base URLs (either by creating separate clients per node or by building full request URLs dynamically) instead of relying on one global base_url in the client[5].
- Global Client State: default_client is a singleton, and tool code doesn’t currently provide a way to switch targets per request. We’ll need to introduce a node selection mechanism inside the client (or a higher-level wrapper) that every tool call automatically uses. Otherwise, tools would continue hitting only the primary node.
- Error Handling Path: Right now, the first network failure triggers a NodeUnreachableError and the tool returns an error without retry[6][11]. In a multi-node scenario, we want to catch that failure and try the next node before bubbling an error up. This means altering _request (or its caller) to attempt another base URL instead of immediately raising NodeUnreachableError. Care must be taken to only retry on genuine connectivity failures – if the node returns a valid HTTP error (like 404 or 500), that means the node was reachable and fallback likely won’t help for that particular request.
- API Key & Admin Endpoints: The MCP may include calls to admin APIs (e.g. /admin/status) that require an API key[12]. Currently, the client attaches the API key from config on those calls (via use_api_key=True)[13]. This approach works for a trusted local node but will not work with public nodes – a public Qortal API will either not have the same API key or will simply refuse admin endpoints. We must ensure that if we fallback to a public node, we do not send the local API key (to avoid leaking secrets), and expect that admin endpoints cannot be fulfilled by public nodes (likely returning Unauthorized). The design should handle this gracefully (e.g. still return an error if a fallback node can’t serve an admin query). For non-admin endpoints, no auth is required, so they are safe to query on public infrastructure.
- Internal Coupling: The current implementation is fairly modular, and introducing a node pool should not require changes in the tool functions’ signatures – they already defer to the client. However, the QortalApiClient and default_client are created early (at import), using env config available at startup. If we allow a list of nodes in config, we need to initialize the NodePool accordingly. We should also verify that no other part of the code caches the base URL or makes direct HTTP calls outside the client. A quick audit suggests all HTTP usage is through QortalApiClient, so centralizing the fallback logic there will cover all tools.
- Typos/Bugs Noticed: Overall, the code is clean. One minor issue spotted is inconsistent parameter naming in the client: for instance, fetch_asset_info uses "assetId" (capital I) for its query parameter[14], but fetch_asset_balances uses "assetid" in lowercase[15]. This may be a typo or an inconsistency – Qortal’s API might accept both, but it should be verified. We should correct such discrepancies (assetId vs assetid) for clarity and consistency. Another minor point: the default_config (and thus base URL and timeouts) is loaded at import; if the environment is changed after startup, it wouldn’t reflect. This is usually fine (config is meant to be static), but it’s something to keep in mind when reloading or testing. No other obvious bugs (like broken references) were found that impact introducing a node pool.

## 2. Design of a NodePool Abstraction

To support fallback to public nodes, we will introduce a NodePool concept that manages multiple Qortal API endpoints with a priority order. The NodePool will always prefer the local node but can fall back to one or more public nodes if the local is unreachable. Key aspects of the design:

- Priority List of Nodes: The NodePool can be initialized with an ordered list of node base URLs. For example: ["http://localhost:12391", "https://api.qortal.org", "https://public-node-2"]. The first entry is the primary (usually localhost), and subsequent entries are fallback public nodes. This list could be configurable via an environment variable or config file (e.g. an env like QORTAL_NODES containing a comma-separated list, or separate QORTAL_PUBLIC_NODES). By default, we might populate it with just the local node, and include Qortal’s official public API (api.qortal.org) as a known fallback. Importantly, we will also include a flag (e.g. ALLOW_PUBLIC_FALLBACK) to enable or disable using any node other than localhost. For security/privacy, it might default to off – only if the operator opts in will the server send requests to public infrastructure. This flag and the node list config should live alongside existing config (likely in qortal_mcp.config as global constants or in the QortalConfig dataclass). For example, we could add to QortalConfig: nodes: list[str] and allow_public_fallback: bool, loaded from env (similar to how DEFAULT_BASE_URL is loaded[1]).
- Node Health Tracking: NodePool should track the health of each node and decide which one to use for new requests. A simple strategy is to always attempt the primary first, and if it fails, try fallbacks in order. However, if the primary is known to be down, we can save time by not always timing out on it. We propose maintaining a basic health status:
- Each node could have a status like healthy or unhealthy, and perhaps a timestamp of when it was marked unhealthy.
- A lightweight health check can be performed to test nodes. A suitable endpoint is GET /blocks/height[16], which returns the current block height (a small, unauthenticated response). This endpoint is trivial for nodes to serve and indicates both connectivity and that the node is caught up (if it returns a plausible height). NodePool could use /blocks/height as a ping: e.g., on startup, ping all nodes to see which are up, or periodically ping unhealthy nodes to see if they recovered.
- Failure Threshold: If a node fails to respond (network error or timeout) perhaps even once, we mark it temporarily unhealthy. Because the MCP server’s requests are read-only and idempotent, a single failure likely means the node is down or unreachable. We might not need multiple failures to decide – even one connection error can trigger trying the next node immediately. However, to avoid flapping, we can implement a cool-down: when a node is marked unhealthy, skip it for some time (e.g. 30 seconds or a configurable period) before retrying it. This prevents every incoming request from uselessly hitting a down node. Alternatively, we only retry the primary periodically (maybe the next request after a cool-down includes a health ping to see if local is back).
- NodePool should not mark a node unhealthy for Qortal-specific errors (like an API returning an error JSON). Only network-level failures (no response, connection refused, timeout) should count. A node that responds with an error (HTTP 400/500) is reachable; those errors likely apply to all nodes (e.g. requesting a non-existent resource will 404 on any node), so trying another node won’t help.
- Node Selection Policy: With health info, the selection works as:
- Primary First: If primary node is healthy (or not marked down), use it.
- If primary is down (either known from previous failures or it fails on this attempt), then iterate through the list of fallback nodes:
- Find the first node that is not marked unhealthy (or try all sequentially if we haven’t marked statuses yet).
- Attempt the request there.
- If it succeeds, great – we may consider that node “active” for this request. We could still prefer the primary on the next request (especially if we plan to periodically re-test it), since local is preferred whenever available for data consistency and to avoid load on public nodes.
- If it fails (network error), mark that node unhealthy and continue to the next.
- If none of the nodes respond, then we treat it as overall Node Unreachable (all nodes down) and propagate the error as we do now.
- API Key Handling: We must integrate authentication carefully:
- The local node likely requires an API key for admin endpoints (our code checks use_api_key for calls like /admin/status and adds the header[13]). We will keep sending the X-API-KEY header for the primary local node as before.
- Do not send the local API key to public nodes. The API key is a secret for the local node and should not leave the machine[17]. NodePool should be aware of which nodes are “trusted local” vs “public”. We can implement this by simply not including the X-API-KEY header on fallback node requests. Concretely, if client._request(... use_api_key=True) is called and we are about to query a non-local node, we skip attaching self.config.api_key for that attempt.
- Impact: If the tool call is for an admin endpoint (status/info/summary/uptime) and we fail over to a public node, that public node will likely return HTTP 401 Unauthorized (since no valid key provided). Our client will catch that and raise UnauthorizedError, which the tool will convert to an error message {"error": "Unauthorized or API key required."}[11]. This is actually the same outcome as if the local node were up but we hadn’t configured the key – so from the client perspective, the error makes sense. It’s arguably better than “Node unreachable” because the node was reachable but refused the request. We should document that some tools (those calling admin APIs) won’t function on public fallback nodes. Optionally, the NodePool could skip trying public nodes for known admin paths to avoid an unnecessary request, but this is a micro-optimization. Simpler is to try and handle the 401 result as we do now.
- Configuration Location: The list of public nodes and the fallback policy flag should reside in configuration for easy adjustment. Options:
- Extend qortal_mcp.config: e.g. PUBLIC_NODES = os.getenv("QORTAL_PUBLIC_NODES", "") which could be parsed into a list. Or add to QortalConfig dataclass as fields with defaults. (Since default_config is created at import, we’d parse the env string then.)
- A boolean like ALLOW_PUBLIC_FALLBACK = os.getenv("QORTAL_ALLOW_PUBLIC_FALLBACK", "false").lower() == "true" to toggle the behavior. By default we might set this to false (opt-in), to preserve the current safe behavior (no external calls) unless explicitly enabled by the operator.
- Alternatively, if a PUBLIC_NODES list is provided and not empty, that alone could imply fallback is desired. But having an explicit flag is clearer and allows shipping a default public node list that’s only used when enabled.
- These settings and their defaults should be documented in README/Design. They align with existing patterns: for example, the code already expects the local node by default and reads env variables for config[18]. We’d do similarly for the node pool.

## 3. Integration Plan for NodePool into the MCP Server

With the above design in mind, here’s how to implement the NodePool and integrate it step-by-step:

3.1 Extend Configuration:
Begin by updating the configuration to include fallback node options: - In qortal_mcp/config.py, add new constants and config fields. For example: - PUBLIC_NODES = os.getenv("QORTAL_PUBLIC_NODES", "") – a comma-separated list of URLs (or empty if none). Parse it into a List[str]. (Alternatively, default to "https://api.qortal.org" if we want to hardcode one public node when none given, but it’s safer to require explicit config for any others). - ALLOW_PUBLIC_FALLBACK = os.getenv("QORTAL_ALLOW_PUBLIC_FALLBACK", "false").lower() == "true". - In the QortalConfig dataclass, add fields public_nodes: list[str] (default from parsing above, or an empty list) and allow_public_fallback: bool (default from env, likely False). Also consider a field for health-check interval or failure threshold if we want it configurable. - The default_config instance will then carry these. This makes them accessible to the client. For instance, default_config.public_nodes might be ["https://api.qortal.org"] if set.

3.2 Implement the NodePool class:
Create a new class (e.g. in a new module qortal_mcp.qortal_api.nodes or within client.py if appropriate) called NodePool. Key responsibilities and structure: - Initialization: Takes a list of node URLs and perhaps a reference to the config/flags. E.g. NodePool(nodes: list[str], allow_public: bool). It will store the list (possibly filter out or separate the primary vs others for convenience). - Clients Storage: Decide how to manage httpx.AsyncClient instances. We have two approaches: 1. Single client, dynamic base URLs: We could use one AsyncClient without a fixed base_url and construct full URLs for each request. However, httpx’s AsyncClient can also be given a base_url per instance. For simplicity and to leverage connection pooling per host, it’s better to create one client per node. NodePool can maintain a dict or list of clients keyed by base URL. This way, each node’s client can keep its own keep-alive connection. 2. NodePool should create these clients on demand (lazy) or at startup. A simple approach: create all clients at init (especially if the node list is short). Use the same timeout and other settings from QortalConfig. For example, httpx.AsyncClient(base_url=node_url, timeout=config.timeout). (Note: If an API key is needed for local, we won’t embed it in the client as a default header; we’ll add it per request as now). - Each client can be stored along with a health status (e.g., in a dict: self.clients[node_url] = {"client": AsyncClient(...), "healthy": True, "fail_count": 0}). - Selecting a Node for Requests: Provide a method like get_client() or get_next_client() that returns an (AsyncClient, base_url) for the next attempt: - It should iterate over the priority list. For each node: - If we have it marked unhealthy, optionally skip if it’s within a recent timeframe of failing. - Otherwise, return its client (creating it if not yet created). - Possibly implement a round-robin after a fail, but since priority is strict (always prefer highest priority available), this is more of a failover than load balancing. - If allow_public_fallback is False, NodePool could either not include any public nodes at all or simply always return only the first (local) node’s client. (Effectively, no fallback). - Marking Unhealthy/Healthy: Provide methods like report_failure(node_url) and report_success(node_url): - On a failure (network error), mark the node as unhealthy (store a timestamp). Also increment a fail counter if we want to remove it after X failures (even one failure might be enough to mark down). - On a success, reset the node’s fail counter and mark it healthy. This is important: if a node was previously down and now responded, we should flag it as healthy again so it can be tried first next time. - Health Check: Optionally, NodePool can have a background task or on-demand check. A pragmatic approach: on each failure of the primary, we could trigger an immediate check of the primary in the background after serving the fallback response – but this might complicate things. Simpler: whenever we are about to use a node marked unhealthy and enough time has passed, attempt a quick /blocks/height call on it (either in-band or out-of-band) to see if it’s back. This can be done within the get_next_client() by doing a blocking (or short-timeout) request before deciding it’s still down. To keep things simple initially, we might skip automatic periodic checks; instead, we rely on trying the primary on each new tool request unless it was very recently failed. In any case, document that the operator can monitor and re-enable the primary node manually (restarting the server or waiting for the next attempt).

3.3 Modify QortalApiClient to use NodePool:
Now integrate NodePool into the existing client logic: - Instantiate NodePool: In QortalApiClient.__init__, if fallback is allowed (e.g. config.allow_public_fallback is True and there are public nodes configured), create a NodePool. For example:

```text
self.node_pool = None
if config and config.allow_public_fallback:
    nodes = [config.base_url] + (config.public_nodes or [])
    self.node_pool = NodePool(nodes, allow_public=True)
```

If fallback is not allowed, we can either not create NodePool (self.node_pool = None) and proceed with the old single-client logic, or still create it but with only the primary (which is effectively the same behavior). Probably simplest is to guard it: no NodePool means operate in single-node mode (for backwards compatibility and possibly to minimize overhead if someone doesn’t want fallback). - Override _get_client and Request Flow: The current _get_client() returns a single AsyncClient (creating it if needed)[5]. We can leave _get_client as-is for single-node mode, but it may not be used when NodePool is active. Instead, focus on _request(): - In _request(self, path, ...): replace the single client.get(...) call with logic to try multiple nodes: 1. If self.node_pool is not set (fallback disabled), simply get the client as before and do one request (preserving current behavior). 2. If NodePool is set: - Loop through NodePool’s available nodes: - Obtain the next node’s AsyncClient and base URL. We might need to construct the full URL or use the client’s base. Since our NodePool’s clients likely have their own base_url configured, we can continue calling client.get(path, ...) as we do now, and it will prepend its base URL internally. - Attach API key header appropriately: If this node’s base URL is the local node (e.g. starts with http://localhost or matches the originally configured base), and use_api_key=True, include the X-API-KEY header as we do now[6]. If it’s a public node, leave out the header (so the request is unauthenticated). - Attempt the GET request. - If the request succeeds (no exception raised): - If we got a valid HTTP response, break out of the loop. (We will still do the usual error-status handling after the loop – i.e. if status >= 400, map to QortalApiError as currently implemented[19]. Those errors propagate normally to the tool, since they are not network failures). - We should also call node_pool.report_success(node_url) to mark the node healthy (especially if it was previously down). - If the request raises an httpx.RequestError (meaning network problem): - Log a warning as currently done (maybe include the node URL in the log for clarity: “Node X unreachable for path Y”). - Mark this node as unhealthy: node_pool.report_failure(node_url). - Continue the loop to try the next node. - End of loop: if we exhausted all nodes without any successful response, then raise NodeUnreachableError("Node unreachable") as we do now (meaning no node in the pool responded). From the tool’s perspective, it will be as if the single node was unreachable, preserving the error schema. - With this approach, the first responding node wins. Important: if a fallback node returns a valid HTTP response (even an error JSON or a 401), we stop and handle it – we do not continue to other nodes because the request was answered. This ensures we don’t, for example, double-fetch or accidentally return data from a secondary node when the primary actually did respond with a valid result or even a deliberate error (like “name not found”). Only true timeouts/connection failures trigger a retry. - Closing Clients: The QortalApiClient.aclose() method is used on server shutdown to close the underlying httpx client[20][21]. We must update this to close all AsyncClient instances in the NodePool: - If NodePool is in use, iterate through all stored clients and call aclose() on each (probably in parallel or sequentially in an async context). - If NodePool is not used, preserve existing behavior (close self._client if it exists)[21]. - After closing, clear out references (set self._client = None and NodePool’s clients to None) to avoid resource leaks. - Ensure Thread/Async Safety: Since FastAPI (ASGI) can handle requests concurrently, our NodePool and client need to handle simultaneous access: - httpx AsyncClient is generally thread-safe for making concurrent requests, but since we will be using it per request within the same async task, that’s fine. The bigger issue is if two requests come in while the primary node is down, both will detect failure and potentially both start iterating fallbacks. This is acceptable, though it might duplicate some attempts. We could consider using an async lock around the NodePool access to avoid thrashing (e.g., ensure only one request at a time performs the failover sequence, maybe caching the result that primary is down), but this might over-complicate for now. Initially, a slightly duplicated effort is fine – the node is down so an extra attempted request by concurrent tasks won’t harm. - Marking nodes unhealthy should ideally be atomic. We can use simple boolean flags and rely on the GIL for the small critical section, or use an asyncio.Lock if we encounter race conditions. Given the low request volume expected (LLM agent calls, not high-frequency public API calls), we can keep it simple. - Tool Layer Adjustments: The tool functions (in tools/*.py) do not need signature changes. By updating the client’s behavior, all tools automatically gain the retry logic. One subtle change: currently, tools sometimes have multiple client calls in one function (e.g. get_account_overview calls fetch_address_info, then fetch_address_balance, then fetch_names_by_owner sequentially[22][23][24]). If the primary node was down, our new logic would failover on the first call (say to api.qortal.org) and succeed. For the second call in the same tool function, should we try the primary again or stick to the one that just worked? Our NodePool as described will still start with primary on each call unless it’s marked down. But after the first failure, we will have marked primary as unhealthy (for at least some time window). So likely the subsequent calls will immediately skip primary and use the now-known working public node (because primary is marked unhealthy). This means within one tool invocation, once we’ve fallen back for one request, the rest will also use the fallback – which is desirable for consistency and performance (don’t keep hitting a down node repeatedly). We should ensure report_failure on primary sets a flag such that further get_next_client() calls treat it as down for a bit. This avoids long timeouts multiple times in one user request. - After some time (or perhaps after the request is done), primary could be retried on the next new tool call. We can implement a short memory (like “mark down for 30s”). Since each tool call is typically short, marking down for even 5-10 seconds would effectively cover the whole multi-call sequence. This can be a simple timestamp check inside NodePool.

- Rate Limits, Timeouts, Headers: We must verify that introducing NodePool doesn’t break other aspects:
- Timeouts: Each AsyncClient will use the same timeout (e.g. 10s) from config[2]. If the primary times out (10s) and then we try a fallback, that could be another 10s, totaling ~20s for a request. That might be acceptable or we may want to consider an overall timeout. Possibly, we could shorten the timeout per node when multiple nodes are configured (e.g. 5s per node) to keep worst-case latency in check. However, to keep things consistent, we might leave it as is unless performance testing suggests otherwise. This could be a tunable policy.
- Headers: Aside from the API key header discussed, we should ensure that httpx.AsyncClient adds no other unexpected headers. We might also copy over any required headers (currently none besides X-API-KEY). If in the future the server needs custom headers (like User-Agent or something), NodePool should not interfere – using separate AsyncClients per node is fine (httpx will by default send a User-Agent: python-httpx which is okay).
- Rate limiting is done at the FastAPI layer per tool (in server.py using rate_limiter)[25][26]. This isn’t affected by which node we call, so no changes needed there.
- Error Mapping: The exception mapping in _request (e.g. turning 404 into QortalApiError with "Resource not found", etc. in _map_error) remains unchanged[19]. Since we will now possibly get errors from different nodes, but all Qortal nodes should return the same error formats, this mapping is still valid. If a public node returns an unexpected message format or HTML (in case of a proxy or outage), our code might throw a generic "Unexpected response" error[27] – which is acceptable as a fallback outcome.

3.4 Testing the Fallback Mechanism:
After implementing, we should extend tests to cover the new behavior: - Simulate a scenario where the primary node’s client raises httpx.RequestError and ensure the client then tries the fallback node. This can be done by injecting a fake AsyncClient for primary that always errors, and a real (or another fake) AsyncClient for the fallback that returns a known response. The combined call should succeed with the fallback’s data. We might add a test case similar to test_node_unreachable_maps_error[28] but expecting that with fallback enabled, no NodeUnreachableError is raised and the data comes through. - Test that if all nodes are down, we still get NodeUnreachableError as before (ensuring we don’t accidentally suppress it). - Test that admin endpoints on fallback yield an Unauthorized error if no key is present. For example, configure NodePool with a secondary node (with no key) and call fetch_node_status(): the result should be an UnauthorizedError (or its mapped JSON error), not NodeUnreachable (because the public node did respond with a 401). This distinguishes a node truly offline vs. online but locked. The tools currently treat both Unauthorized and NodeUnreachable as different error messages[29], so it’s important our logic raises the correct one. In practice, our _request already does this: if the fallback node returns 401, _request will raise UnauthorizedError[30], which the tool converts to {"error": "Unauthorized or API key required."}[11]. - Test that partial failures in a multi-call tool function are handled: e.g., if primary fails on first sub-call but succeeds on second (perhaps it rebooted mid-call?). This is an edge case – our marking logic might skip retrying primary so quickly, which is fine. We could simulate a scenario where NodePool marks primary down and ensure subsequent calls in the same function use fallback without additional delay.

3.5 Documentation and Configuration Updates:
Document the new feature in README or DESIGN: - Note that the server can be configured to use multiple Qortal nodes. Provide examples of setting env vars: e.g., QORTAL_PUBLIC_NODES="https://api.qortal.org" and QORTAL_ALLOW_PUBLIC_FALLBACK=true to enable the feature. - Explain that if the local node is offline, the server will query public nodes for data. Include a caution that some data (like node status) may not be available via public nodes (since they might require API keys or simply reflect a different node’s status). - Perhaps note that using public nodes means trusting them to provide accurate blockchain data. For read-only queries this is generally fine, but if absolute trust is required (e.g. for verifying balances), the local node is authoritative. This is more of an advisory note.

## 4. Additional Considerations and Cleanup

Before implementing NodePool, it’s worth addressing a few cleanup items: - Inconsistencies: Fix the parameter naming inconsistency (assetid vs assetId in the client) to use a uniform style (preferably the camelCase expected by Qortal API, e.g., assetId)[15]. While Qortal might accept lowercase query keys, using the documented format is less confusing. - Typos/Comments: Check docstrings and log messages for any minor typos. (For example, British vs American spelling of “Synchronising” is handled in code[31] – that’s fine and intentional.) - Testing NodePool: Add unit tests for NodePool’s node selection logic (simulate marking nodes up/down). Also add integration tests that can run against a real local node and a known public node (perhaps guarded by an env flag like LIVE_QORTAL_PUBLIC=1) to ensure the fallback works in practice. - Performance: Monitor the behavior under various failover scenarios. If the local node is down, each request will incur at least one failure (with timeout) before succeeding on a public node – that could slow responses. Consider lowering the timeout for primary if public fallback is enabled (for example, try local with a 3s timeout, then public with 10s). This could be a future enhancement; initially, we keep it simple with the same timeout and rely on marking unhealthy after one try to avoid repeating the delay. - Security: Double-check that no sensitive information is leaked to public nodes. The main concern is the API key (addressed by not forwarding it). Also ensure we don’t forward any other local-only info. All requests are GET and publicly safe data by design, so this should be fine.

By following this plan – auditing current assumptions, introducing a robust NodePool, and carefully integrating it – we will achieve a Qortal MCP server that preferentially uses the local Qortal Core but can seamlessly fall back to public API nodes when needed. This increases the service’s reliability for read-only queries, while still maintaining security (no writes, and controlled use of the API key). The above steps provide a ready roadmap for a coder to implement and verify the feature, along with noting a few areas for polish during the refactor.

[1] [2] [3] [4] config.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/config.py

[5] [6] [7] [8] [13] [14] [15] [16] [19] [27] [30] client.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/qortal_api/client.py

[9] names.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/tools/names.py

[10] [22] [23] [24] account.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/tools/account.py

[11] [29] [31] node.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/tools/node.py

[12] [17] DESIGN.md

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/DESIGN.md

[18] README.md

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/README.md

[20] [25] [26] server.py

https://github.com/QuickMythril/qortal-mcp-server/blob/5cf26d3b5e2fe06151c4301167f0062af9154459/qortal_mcp/server.py

[21] [28] test_client_unreachable.py

https://github.com/QuickMythril/qortal-mcp-server/blob/e6ac1024debd0a4b32782ee65bf52b737c47dcd8/tests/test_client_unreachable.py

## Implementation Checklist / Roadmap

- [x] Config extensions
  - [x] Add `public_nodes` (parsed from `QORTAL_PUBLIC_NODES`) and `allow_public_fallback` (from `QORTAL_ALLOW_PUBLIC_FALLBACK`) to `QortalConfig`, with sane defaults (fallback off).
  - [x] Document defaults and examples in `README.md`/`DESIGN.md`; note trust implications of public nodes.
- [x] Client groundwork
  - [x] Normalize any query param inconsistencies (e.g., `assetId` vs `assetid`) per Qortal API docs.
  - [x] Ensure current single-client path remains intact when fallback is disabled.
- [x] NodePool implementation
  - [x] Create NodePool to hold ordered node list (primary + public), per-node `AsyncClient`s, and health state with cooldown timestamps.
  - [x] Implement success/failure reporting to mark nodes healthy/unhealthy; skip recently failed nodes until cooldown expires.
  - [x] Add optional lightweight health check target (e.g., `/blocks/height`) guarded by existing endpoint whitelist and rate-limit concerns.
- [x] Request routing changes
  - [x] Update `_request` to iterate nodes on network errors only (connection/timeout); stop on first real HTTP response (even 4xx/5xx).
  - [x] Attach API key only for trusted/local node; never forward key to public nodes.
  - [x] Preserve backward-compatible single-node behavior when `allow_public_fallback` is false.
- [x] Shutdown/cleanup
  - [x] Update `aclose` to close all NodePool clients; clear references.
- [x] Tool behavior verification
  - [x] Confirm multi-call tools reuse a healthy fallback within the same invocation after primary is marked down.
  - [x] Consider short per-node timeout adjustments or documented guidance to keep total latency bounded during failover.
- [x] Tests
  - [x] Unit tests for NodePool selection/health marking and client failover (primary network error -> fallback success).
  - [x] Tests for all-nodes-down path (still raises NodeUnreachable) and admin endpoint on public node returning Unauthorized.
  - [x] Maintain/update existing unreachable tests; add fixtures/mocks for multi-call tool behavior.
- [x] Documentation
  - [x] Update `README.md`/`DESIGN.md` with configuration flags, examples, and limitations (admin endpoints may fail on public nodes; trust caveats).
  - [x] Note any timeout/cooldown defaults and health-check behavior; reiterate rate-limit considerations.
